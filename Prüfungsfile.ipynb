{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec6eb192",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f524d0bd",
   "metadata": {},
   "source": [
    "### Syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ead40c",
   "metadata": {},
   "source": [
    "- Entstehung von Struktur zwischen Wörtern (=Syntax) mit eigener Bedeutung\n",
    "(über die Summe der Teile hinaus)\n",
    "- Einordnung und Struktur\n",
    "- Einordnung oder Kategorisierung von Sätzen, Phrasen, Wörtern usw.\n",
    "- Strukturierung/Ordnung/Reihenfolge: Kombination zu jeweils längeren Blöcken\n",
    "- Gesucht wird Strukturierung der Wörter.\n",
    "- Oft: Hierarchische Strukturierung\n",
    "• Satz\n",
    "• Satzteil (Clause)\n",
    "• Phrase\n",
    "• Wort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e4c539",
   "metadata": {},
   "source": [
    "### Semantik"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b97d93",
   "metadata": {},
   "source": [
    "- Bedeutung von Sprache\n",
    "- Lexikalische Semantik:               Bedeutung von Wörtern oder Symbolen\n",
    "- Kompositionelle Semantik:            Beziehungen zwischen Wörtern\n",
    "- Bedeutung von Wortfolgen und Sätzen\n",
    "- Beziehungen zwischen Sätzen\n",
    "\n",
    "#### Morphem:\n",
    "\n",
    "- Baut oft statt auf Wörtern auf Morphemen auf\n",
    "- Kleinste bedeutungsvolle Einheit\n",
    "- Wort kann aus mehreren Morphemen bestehen\n",
    "- Zusammengesetzte Wörter\n",
    "- Vor- und Nachsilben\n",
    "- Ergänzungen für Plural usw.\n",
    "- Bedeutung ergibt sich aus der Kombination der Einzelbedeutungen:\n",
    "- Beispiel: Tische: {Tisch}{e} -> bestimmtes Möbelstück in der Mehrzahl\n",
    "\n",
    "#### Lexem:\n",
    "- Sprachliche Bedeutungseinheit\n",
    "- Abstrahiert von Aussprache, Syntax usw.\n",
    "- Beispiel: Flexionsformen eines Verbs gehören zum selben Lexem singen, singt, singst\n",
    "- (uneinheitliche Definitionen)\n",
    "\n",
    "#### Lemma:\n",
    "- Grundform, Lexikoneintrag\n",
    "- Für deutsche Nomen bspw. Nominativ Singular: Lied, Baum, Vorlesung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e292ab9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad064c30",
   "metadata": {},
   "source": [
    "### Grammatik"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2c23ea",
   "metadata": {},
   "source": [
    "Syntactic categories (common denotations) in NLP\n",
    "\n",
    "    np - noun phrase\n",
    "    vp - verb phrase\n",
    "    s - sentence\n",
    "    det - determiner (article)\n",
    "    n - noun\n",
    "    tv - transitive verb (takes an object)\n",
    "    iv - intransitive verb\n",
    "    prep - preposition\n",
    "    pp - prepositional phrase\n",
    "    adj - adjective "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3d20e7",
   "metadata": {},
   "source": [
    "    Tag \tMeaning \t            English Examples\n",
    "    ADJ \tadjective \t            new, good, high, special, big, local\n",
    "    ADP \tadposition \t            on, of, at, with, by, into, under\n",
    "    ADV \tadverb  \t            really, already, still, early, now\n",
    "    CONJ \tconjunction \t        and, or, but, if, while, although\n",
    "    DET \tdeterminer, article \tthe, a, some, most, every, no, which\n",
    "    NOUN \tnoun \t                year, home, costs, time, Africa\n",
    "    NUM \tnumeral \t            twenty-four, fourth, 1991, 14:24\n",
    "    PRT \tparticle \t            at, on, out, over per, that, up, with\n",
    "    PRON \tpronoun \t            he, their, her, its, my, I, us\n",
    "    VERB \tverb \t                is, say, told, given, playing, would\n",
    "    . \t    punctuation marks \t    . , ; !\n",
    "    X    \tother \t                ersatz, esprit, dunno, gr8, univeristy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c96ed8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP\n",
      "    (DET the)\n",
      "    (ADJ lazy)\n",
      "    (N cat)\n",
      "    (PP (PREP in) (NP (DET a) (N box)))))\n",
      "(S\n",
      "  (NP\n",
      "    (DET the)\n",
      "    (ADJ lazy)\n",
      "    (N cat)\n",
      "    (PP (PREP in) (NP (DET a) (N box)))))\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Zusatzuebung aus Termin 1 NLP aus Vorlesung\n",
    "Erweitern Sie die Grammatik um Haupt - und Nebensatz.\n",
    "Beispielsatz: the brown fox is quick and he is jumping over the lazy dog\n",
    "\n",
    "https://www.nltk.org/howto/grammar.html\n",
    "\"\"\"\n",
    "\n",
    "import nltk\n",
    "\n",
    "#Start\n",
    "grammar2 = nltk.CFG.fromstring(\"\"\"\n",
    "  S  -> NP | NP VP\n",
    "  NP -> DET ADJ N PP | DET N | DET ADJ N | DET ADJ N PP \n",
    "  VP -> V\n",
    "  PP -> PREP NP\n",
    "  N -> 'fox' | 'dog' | 'cat' | 'box'\n",
    "  DET -> 'the' | 'a'\n",
    "  ADJ  -> 'quick' | 'brown' |  'lazy' | 'tall'\n",
    "  V ->  'is'  | 'jumping' | 'running'\n",
    "  PREP -> 'in'\n",
    "  \"\"\")\n",
    "\n",
    "# words = \"the brown fox\".split()\n",
    "words = \"the lazy cat in a box\".split()\n",
    "my_parser = nltk.RecursiveDescentParser(grammar2)  # (grammar2, trace=2) falls keine Ausgabe kommt...\n",
    "for tree in my_parser.parse(words):\n",
    "    print(tree)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820bbe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Erweitern sie das Programmierbeispiel von eben, indem sie weitere Sätze finden, \n",
    "die von der unveränderten Grammatik geparst werden können. Finden sie mindestens \n",
    "drei weitere Sätze und stellen sie diese mit dem Parse-Baum hier ein.\n",
    "\n",
    "Tipp: Wenn keine Ausgabe kommt, ist ein Parse-Fehler aufgetreten. \n",
    "Sie können wie im Python-Beispiel auskommentiert das tracing einschalten, um mehr Hinweise zu bekommen.\n",
    "\"\"\"\n",
    "\n",
    "grammar2 = nltk.CFG.fromstring(\"\"\"\n",
    "  S  -> NP | NP VP PP | NP VP\n",
    "  NP -> DET ADJ N PP | DET N | DET ADJ N \n",
    "  VP -> V | V V\n",
    "  PP -> PREP NP\n",
    "  N -> 'fox' | 'dog' | 'cat' | 'box'\n",
    "  DET -> 'the' | 'a'\n",
    "  ADJ  -> 'quick' | 'brown' |  'lazy' | 'tall'\n",
    "  V ->  'is'  | 'jumping' | 'running'\n",
    "  PREP -> 'in'\n",
    "  \"\"\")\n",
    "# words = \"the brown fox\".split()\n",
    "#words = \"the brown fox in a box\".split()\n",
    "#words = \"a quick cat running\".split()\n",
    "#words = \"the quick fox is in the brown box\".split()\n",
    "words = \"the quick fox is jumping\".split()\n",
    "my_parser = nltk.RecursiveDescentParser(grammar2) # (grammar2, trace=2) falls keine Ausgabe kommt...\n",
    "for tree in my_parser.parse(words):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b27a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Zusatzuebung aus Termin 1 NLP aus Vorlesung\n",
    "Erweitern Sie die Grammatik um Haupt - und Nebensatz.\n",
    "Beispielsatz: the brown fox is quick and he is jumping over the lazy dog\n",
    "\n",
    "https://www.nltk.org/howto/grammar.html\n",
    "\"\"\"\n",
    "grammar2 = nltk.CFG.fromstring(\"\"\" \n",
    "  S   -> HS CCNJ NS | HS\n",
    "  HS  -> NP | NP VP | NP VP PP | NP VP ADJ\n",
    "  NS  -> PRP VP PREP NP\n",
    "  NP -> DET ADJ N PP | DET N | DET ADJ N\n",
    "  PRP -> PR\n",
    "  VP -> V | V V\n",
    "  PP -> PREP NP\n",
    "  N -> 'fox' | 'dog' | 'cat' | 'box'\n",
    "  DET -> 'the' | 'a'\n",
    "  ADJ  -> 'quick' | 'brown' |  'lazy' | 'tall' | \"blue\"\n",
    "  V ->  'is'  | 'jumping' | 'running' | \"jumps\"\n",
    "  PREP -> 'in' | \"over\"\n",
    "  PR  -> \"he\" | \"she\" | \"it\"\n",
    "  CCNJ -> \"and\" | \"but\" | \"that\" | \"if\"\n",
    "  \"\"\")\n",
    "words = \"the brown fox is quick and he is jumping over the lazy dog\".split()\n",
    "my_parser = nltk.RecursiveDescentParser(grammar2)  # (grammar2, trace=2) falls keine Ausgabe kommt...\n",
    "for tree in my_parser.parse(words):  # man muss nicht unbedingt alle trees finden, manchmal genügt auch einer!!\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eb45b9",
   "metadata": {},
   "source": [
    "### Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f29ac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67182c93",
   "metadata": {},
   "source": [
    "Syntax\n",
    "https://docs.python.org/3/library/re.html#regular-expression-syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f3c83e",
   "metadata": {},
   "source": [
    "- \\d Matches any decimal digits (0,1,2,3,4,5,6,7,8,9).\n",
    "- \\D Matches any character that is not a decimal digit.\n",
    "- \\s Matches any whitespace character such as space, tab and newline.\n",
    "- \\S Matches any character that is not a whitespace.\n",
    "- \\w Matches any \"word\" character (letter, digit or the underscore).\n",
    "- \\W Matches any character that is not a \"word\" character.\n",
    "- [...] Matches any character that listed inside square brackets. For example [abc] matches one character that is either a, b or c.\n",
    "- [^...] Matches any character that is not listed inside square brackets. For example [^abc] matches one character that is NOT a, b or c.\n",
    "- [0-9] Matches any character between 0 and 9. It is equivalent to using \\d.\n",
    "- [A-D] Matches any character between A and F (A, B, C, D).\n",
    "\n",
    "\n",
    "- \" + \" Matches the previous item one or more times. For example, A+ will match strings such as A, AA, AAA and etc.\n",
    "- \" ? \" Matches the previous item zero or one time. It is used to match an optional part of the pattern.\n",
    "- \" * \" Matches the previous item zero or more time. It is used to match optional parts of the pattern.\n",
    "- {n} Matches the previous item exactly n times. For example, A{2} will match strings such as AA.\n",
    "- {n,m} Matches the previous item at least n times, but no more than m times. For example, A{2,5} will match strings such as AA, AAA, AAAA or AAAAA.\n",
    "- {n,} Matches the previous item at least n or more times. For example, A{2,} will match strings such as AA, AAA, AAAA, AAAAA and so on.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f509d86",
   "metadata": {},
   "source": [
    "### Übung Telefonnummern aus Text extrahieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5039a243",
   "metadata": {},
   "source": [
    "Probieren sie die Beispiele zu regulären Ausdrücken aus (Code von Sarkar, Ch2b).\n",
    "Bilden sie einen regulären Ausdruck für Telefonnummern und testen sie diesen mittels selbstgewählter Beispiele. \n",
    "Sie sollen als Ziel (potentielle) Telefonnummern in beliebigem anderem Text erkennen und ausgeben können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78fc404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "string = \"Hallo, meine Telefonnummer ist 0765227843  [Titel anhand dieser ISBN in Citavi-Projekt übernehmen]  . International ist es +41765227843 blah +41 76 522 78 43 Ich wohne in 0800 Zürich. Foo bar \" \\\n",
    "         \"076 522 78 43, blah 435345300555 +41(0)765227843 und 0041765527843\"\n",
    "\n",
    "pattern = '0+\\d{2}\\s?\\d{3}\\s?\\d{2}\\s?\\d{2}|\\+\\d{2}\\s?\\d{2}\\s?\\d{3}\\s?\\d{2}\\s?\\d{2}'\n",
    "pattern2 = '(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})'\n",
    "pattern3 = '((?:\\+\\d{2}[-\\.\\s]??|\\d{4}[-\\.\\s]??)?(?:\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4}))'\n",
    "\n",
    "\n",
    "print(re.findall(pattern, string))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c533a4b",
   "metadata": {},
   "source": [
    "### Übungsaufgabe Web-Scraping\n",
    "\n",
    "Wir wollen das Web-Scraping auf einen eigenen Text anwenden und diesen entsprechend dem Vorbild aufbereiten. Dazu wählen wir die Bündner Verfassung. Diese ist beispielsweise hier erreichbar: http://www.verfassungen.ch/graubuenden/verf2003.htm (Bitte verwenden sie diese Quelle, auch wenn sie eine andere kennen sollten, damit wir alle auf der gleichen Grundlage arbeiten).\n",
    "\n",
    "    Quelltext/HTML herunterladen\n",
    "    Reinen Text extrahieren\n",
    "    (gut aufbewahren, brauchen wir noch...)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d71ae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text) #löscht Umbrüche und so Sachen\n",
    "    return stripped_text\n",
    "\n",
    "data = requests.get('https://gastrosocial.ch/de/arbeitgeber/versicherungsangebot/uebersicht-versicherungsangebot')\n",
    "content = data.content\n",
    "\n",
    "\n",
    "\n",
    "clean_content = strip_html_tags(content)\n",
    "print(clean_content[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31420de",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3980162",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9e7cb7",
   "metadata": {},
   "source": [
    "#### Verfahren:\n",
    "- Porter Stemmer\n",
    "- Lancaster Stemmer\n",
    "- Regex based Stemmer\n",
    "- Snowball Stemmer\n",
    "##### Vor/Nachteile\n",
    "- Vorteil: regelbasiert, einfach, muss kein Wörterbuch aufgebaut werden\n",
    "- Nachteil: ungenauer als Lemmatisierung\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a5f30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porter Stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "print(ps.stem('jumping'), ps.stem('jumps'), ps.stem('jumped'))\n",
    "print(ps.stem('lying'))\n",
    "print(ps.stem('strange'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96991a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lancaster Stemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "ls = LancasterStemmer()\n",
    "\n",
    "print(ls.stem('jumping'), ls.stem('jumps'), ls.stem('jumped'))\n",
    "print(ls.stem('lying'))\n",
    "print(ls.stem('strange'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c03a595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex based stemmer\n",
    "from nltk.stem import RegexpStemmer\n",
    "rs = RegexpStemmer('ing$|s$|ed$', min=4)\n",
    "print(rs.stem('jumping'), rs.stem('jumps'), rs.stem('jumped'))\n",
    "print(rs.stem('lying'))\n",
    "print(rs.stem('strange'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef73d959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowball Stemmer (auch für Deutsch, aber schneided nur Endung ab, \n",
    "# Stemmming geht nicht gut in Deutsch, besser lemmatisierung)\n",
    "from nltk.stem import SnowballStemmer\n",
    "ss = SnowballStemmer(\"german\")\n",
    "print('Supported Languages:', SnowballStemmer.languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c2b0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming on German words\n",
    "# autobahnen -> cars\n",
    "# autobahn -> car\n",
    "print(ss.stem('autobahnen'))\n",
    "\n",
    "# springen -> jumping\n",
    "# spring -> jump\n",
    "print(ss.stem('springen'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5281f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_stemmer(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "simple_stemmer(\"My system keeps crashing his crashed yesterday, ours crashes daily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e225b35b",
   "metadata": {},
   "source": [
    "### Lemmatisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef83751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf66de5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize nouns\n",
    "print(wnl.lemmatize('cars', 'n'))\n",
    "print(wnl.lemmatize('men', 'n'))\n",
    "\n",
    "# lemmatize verbs\n",
    "print(wnl.lemmatize('running', 'v'))\n",
    "print(wnl.lemmatize('ate', 'v'))\n",
    "\n",
    "# lemmatize adjectives\n",
    "print(wnl.lemmatize('saddest', 'a'))\n",
    "print(wnl.lemmatize('fancier', 'a'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e3bd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# use spacy.load('en') if you have downloaded the language model en directly after install spacy\n",
    "nlp = spacy.load('en_core', parse=True, tag=True, entity=True)\n",
    "text = 'My system keeps crashing his crashed yesterday, ours crashes daily'\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "lemmatize_text(\"My system keeps crashing! his crashed yesterday, ours crashes daily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e391491",
   "metadata": {},
   "source": [
    "### Stopwörter entfernen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbb7cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text, is_lower_case=False, stopwords=stopword_list):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "remove_stopwords(\"The, and, if are stopwords, computer is not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5b5ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = (\"US unveils world's most powerful supercomputer, beats China. \" \n",
    "               \"The US has unveiled the world's most powerful supercomputer called 'Summit', \" \n",
    "               \"beating the previous record-holder China's Sunway TaihuLight. With a peak performance \"\n",
    "               \"of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, \"\n",
    "               \"which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, \"\n",
    "               \"which reportedly take up the size of two tennis courts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eff8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "remove_accented_chars('Sómě Áccěntěd těxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3d5a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contractions import CONTRACTION_MAP\n",
    "import re\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48abe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=True, \n",
    "                     text_lemmatization=True, special_char_removal=True, \n",
    "                     stopword_removal=True, remove_digits=True):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "        # strip HTML\n",
    "        if html_stripping:\n",
    "            doc = strip_html_tags(doc)\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "        # expand contractions    \n",
    "        if contraction_expansion:\n",
    "            doc = expand_contractions(doc)\n",
    "        # lowercase the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        # remove extra newlines\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "        # remove special characters and\\or digits    \n",
    "        if special_char_removal:\n",
    "            # insert spaces between special characters to isolate them    \n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "            \n",
    "        normalized_corpus.append(doc)\n",
    "        \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5391ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'Original': sample_text,\n",
    " 'Processed': normalize_corpus([sample_text])[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1965a208",
   "metadata": {},
   "source": [
    "### Übung Normalisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6ec37a",
   "metadata": {},
   "source": [
    "Wenden sie den kompletten Normalisierungsablauf auf unseren Beispieltext an (Bündner Verfassung).\n",
    "\n",
    "    Gehen sie schrittweise vor und überprüfen sie, ob es Änderungen gibt und falls ja, welche.\n",
    "    Ändern sie die Einstellungen, wo notwendig oder sinnvoll\n",
    "    Dokumentieren sie ihre Schritte \n",
    "    Wo ist noch Verbesserungsbedarf?\n",
    "    Insbesondere natürlich für die Umstellung von Englisch auf Deutsch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8a884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"de_core_news_sm\")  # für Deutsch statt Englisch (en_core_web_sm) inkl. Installation\n",
    "verfassung = content  # Eindeutige Bezeichnung für unsere Textdaten\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "print(lemmatize_text(\"My system keeps crashing! his crashed yesterday, ours crashes daily\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363e8009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False, stopwords=stopword_list):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "print(remove_stopwords(\"The, and, if are stopwords, computer is not\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f4e823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=True,\n",
    "                     text_lemmatization=True, special_char_removal=True,\n",
    "                     stopword_removal=True, remove_digits=True):\n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "        # strip HTML\n",
    "        if html_stripping:\n",
    "            doc = strip_html_tags(doc)\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "        # expand contractions\n",
    "        if contraction_expansion:\n",
    "            doc = expand_contractions(doc)\n",
    "        # lowercase the text\n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        # remove extra newlines\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ', doc)\n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "        # remove special characters and\\or digits\n",
    "        if special_char_removal:\n",
    "            # insert spaces between special characters to isolate them\n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits)\n",
    "            # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "        normalized_corpus.append(doc)\n",
    "    return normalized_corpus\n",
    "\n",
    "sample_text = verfassung  # Abruf des deutschen Texts\n",
    "print('Original:', sample_text)\n",
    "print('Normalisiert:', normalize_corpus([sample_text])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0306fc0",
   "metadata": {},
   "source": [
    "### Pos Tagger (Wortarterkennung) (NLP5.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa07873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "sentence = \"US unveils world's most powerful supercomputer, beats China.\"\n",
    "nltk_pos_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "print(f\"Beispielsatz getaggt: {nltk_pos_tagged}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4411ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank\n",
    "data = treebank.tagged_sents()\n",
    "train_data = data[:3500]\n",
    "test_data = data[3500:]\n",
    "print(f\"Satz aus den Trainingsdaten: {train_data[0]}\")\n",
    "print(f\"Anzahl Sätze insgesamt: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3616a769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default tagger\n",
    "from nltk.tag import DefaultTagger\n",
    "dt = DefaultTagger('NN')#nur Nomen...\n",
    "\n",
    "# accuracy on test data - wir nutzen die eingebaute Methode evaluate...\n",
    "print(f\"Genauigkeit nur Nomen: {dt.evaluate(test_data)}\")\n",
    "\n",
    "# tagging our sample headline\n",
    "print(f\"Angewendet auf Beispielsatz: {dt.tag(nltk.word_tokenize(sentence))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73f2969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import RegexpTagger\n",
    "# define regex tag patterns\n",
    "patterns = [\n",
    "        (r'.*ing$', 'VBG'),               # gerunds\n",
    "        (r'.*ed$', 'VBD'),                # simple past\n",
    "        (r'.*es$', 'VBZ'),                # 3rd singular present\n",
    "        (r'.*ould$', 'MD'),               # modals\n",
    "        (r'.*\\'s$', 'NN$'),               # possessive nouns\n",
    "        (r'.*s$', 'NNS'),                 # plural nouns\n",
    "        (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers\n",
    "        (r'.*', 'NN')                     # nouns (default) ...\n",
    "]\n",
    "rt = RegexpTagger(patterns)\n",
    "# accuracy on test data\n",
    "print(f\"Genauigkeit Regex: {rt.evaluate(test_data)}\")\n",
    "# tagging our sample headline\n",
    "print(f\"Regex auf Beispielsatz: {rt.tag(nltk.word_tokenize(sentence))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b4790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## N gram taggers\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger\n",
    "from nltk.tag import TrigramTagger\n",
    "\n",
    "ut = UnigramTagger(train_data)\n",
    "bt = BigramTagger(train_data)\n",
    "tt = TrigramTagger(train_data)\n",
    "# testing performance of unigram tagger\n",
    "print(f\"Genauigkeit Unigram: {ut.evaluate(test_data)}\")\n",
    "print(f\"Beispielsatz: {ut.tag(nltk.word_tokenize(sentence))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5fad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing performance of bigram tagger\n",
    "print(f\"Genauigkeit Bigram: {bt.evaluate(test_data)}\")\n",
    "print(f\"Beispielsatz: {bt.tag(nltk.word_tokenize(sentence))}\")\n",
    "input(\"Warte...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4969f8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing performance of trigram tagger\n",
    "print(f\"Genauigkeit Trigram: {tt.evaluate(test_data)}\")\n",
    "print(f\"Beispielsatz: {tt.tag(nltk.word_tokenize(sentence))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be2ebeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_tagger(train_data, taggers, backoff=None):\n",
    "    for tagger in taggers:\n",
    "        backoff = tagger(train_data, backoff=backoff)\n",
    "    return backoff\n",
    "\n",
    "ct = combined_tagger(train_data=train_data,\n",
    "                     taggers=[UnigramTagger, BigramTagger, TrigramTagger],\n",
    "                     backoff=rt)\n",
    "\n",
    "\n",
    "# evaluating the new combined tagger with backoff taggers\n",
    "print(f\"Genauigkeit Combined: {ct.evaluate(test_data)}\")\n",
    "print(f\"Beispielsatz: {ct.tag(nltk.word_tokenize(sentence))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ec2575",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.tag.sequential import ClassifierBasedPOSTagger\n",
    "\n",
    "nbt = ClassifierBasedPOSTagger(train=train_data,\n",
    "                               classifier_builder=NaiveBayesClassifier.train)\n",
    "\n",
    "# evaluate tagger on test data and sample sentence\n",
    "print(f\"Genauigkeit ML/Bayes: {nbt.evaluate(test_data)}\")\n",
    "print(f\"Beispielsatz: {nbt.tag(nltk.word_tokenize(sentence))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5d5e95",
   "metadata": {},
   "source": [
    "### Shallow Parsing (NLP5.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caffe72d",
   "metadata": {},
   "source": [
    "#### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "412bd05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beispielsatz aus Trainingsdaten: (S\n",
      "  (NP A/DT Lorillard/NNP spokewoman/NN)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  ``/``\n",
      "  (NP This/DT)\n",
      "  is/VBZ\n",
      "  (NP an/DT old/JJ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# Shallow Parsing or Chunking\n",
    "import nltk\n",
    "from nltk.corpus import treebank_chunk\n",
    "\n",
    "sentence = \"US unveils world's most powerful supercomputer, beats China.\"\n",
    "\n",
    "data = treebank_chunk.chunked_sents()\n",
    "\n",
    "train_data = data[:3500]\n",
    "test_data = data[3500:]\n",
    "# view sample data\n",
    "print(f\"Beispielsatz aus Trainingsdaten: {train_data[7]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8c3ffb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags: [('US', 'NNP'), ('unveils', 'JJ'), ('world', 'NN'), (\"'s\", 'POS'), ('most', 'RBS'), ('powerful', 'JJ'), ('supercomputer', 'NN'), (',', ','), ('beats', 'VBZ'), ('China', 'NNP'), ('.', '.')]\n",
      "Regex NP chunking example: \n",
      "(S\n",
      "  (NP US/NNP)\n",
      "  (NP unveils/JJ world/NN)\n",
      "  's/POS\n",
      "  most/RBS\n",
      "  (NP powerful/JJ supercomputer/NN)\n",
      "  ,/,\n",
      "  beats/VBZ\n",
      "  (NP China/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.chunk import RegexpParser\n",
    "\n",
    "# get POS tagged sentence\n",
    "tagged_simple_sent = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "print('POS Tags:', tagged_simple_sent)\n",
    "\n",
    "# illustrate NP chunking based on explicit chunk patterns\n",
    "chunk_grammar = \"\"\"\n",
    "NP: {<DT>?<JJ>*<NN.*>}\n",
    "\"\"\"\n",
    "rc = RegexpParser(chunk_grammar)\n",
    "c = rc.parse(tagged_simple_sent)\n",
    "\n",
    "# print and view chunked sentence using chunking\n",
    "print(f\"Regex NP chunking example: \\n{c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128b53a1",
   "metadata": {},
   "source": [
    "#### Chinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "722b610e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk/chink example:\n",
      "(S\n",
      "  (NP US/NNP)\n",
      "  unveils/JJ\n",
      "  (NP world/NN 's/POS most/RBS)\n",
      "  powerful/JJ\n",
      "  (NP supercomputer/NN ,/,)\n",
      "  beats/VBZ\n",
      "  (NP China/NNP ./.))\n"
     ]
    }
   ],
   "source": [
    "# illustrate NP chunking based on explicit chink patterns\n",
    "chink_grammar = \"\"\"\n",
    "NP:\n",
    "    {<.*>+}             # Chunk everything as NP\n",
    "    }<VBZ|VBD|JJ|IN>+{  # Chink sequences of VBD/VBZ/JJ/IN\n",
    "\"\"\"\n",
    "rc = RegexpParser(chink_grammar)\n",
    "c = rc.parse(tagged_simple_sent)\n",
    "\n",
    "# print and view chunked sentence using chinking\n",
    "print(f\"Chunk/chink example:\\n{c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f89fc5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mehr Phrasen:\n",
      "(S\n",
      "  (NP US/NNP)\n",
      "  (NP unveils/JJ world/NN)\n",
      "  's/POS\n",
      "  (ADVP most/RBS)\n",
      "  (NP powerful/JJ supercomputer/NN)\n",
      "  ,/,\n",
      "  (VP beats/VBZ)\n",
      "  (NP China/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# create a more generic shallow parser\n",
    "grammar = \"\"\"\n",
    "NP: {<DT>?<JJ>?<NN.*>}  \n",
    "ADJP: {<JJ>}\n",
    "ADVP: {<RB.*>}\n",
    "PP: {<IN>}      \n",
    "VP: {<MD>?<VB.*>+}\n",
    "\"\"\"\n",
    "rc = RegexpParser(grammar)\n",
    "c = rc.parse(tagged_simple_sent)\n",
    "\n",
    "# print and view shallow parsed sample sentence\n",
    "print(f\"Mehr Phrasen:\\n{c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b3c6b0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation auf Testdaten: \n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  46.1%%\n",
      "    Precision:     19.9%%\n",
      "    Recall:        43.3%%\n",
      "    F-Measure:     27.3%%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate parser performance on test data\n",
    "print(f\"Evaluation auf Testdaten: \\n{rc.evaluate(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e8bbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conll - Dateiformat im NLP, geht auf Conference on Natural Language Learning zurück\n",
    "from nltk.chunk.util import tree2conlltags, conlltags2tree\n",
    "\n",
    "# look at a sample training tagged sentence\n",
    "train_sent = train_data[7]\n",
    "print(f\"Nochmal unser Beispielsatz aus den Trainingsdaten:\\n{train_sent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd27eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the (word, POS tag, Chunk tag) triples for each token\n",
    "wtc = tree2conlltags(train_sent)\n",
    "print(f\"Tripel aus Wort, Tag, Chunk-Tag für den Beispielsatz:\\n{wtc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d60f465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get shallow parsed tree back from the WTC triples\n",
    "tree = conlltags2tree(wtc)\n",
    "print(f\"Zusammenführung der Tripel zu Parse-Baum:\\n{tree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c730009a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tags und chunk-tags extrahieren, wörter verwerfen\n",
    "def conll_tag_chunks(chunk_sents):\n",
    "    tagged_sents = [tree2conlltags(tree) for tree in chunk_sents]\n",
    "    return [[(t, c) for (w, t, c) in sent] for sent in tagged_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6732539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tagger sequentiell kombinieren (wie im POS-Beispiel)\n",
    "def combined_tagger(train_data, taggers, backoff=None):\n",
    "    for tagger in taggers:\n",
    "        backoff = tagger(train_data, backoff=backoff)\n",
    "    return backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446f6b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import UnigramTagger, BigramTagger\n",
    "from nltk.chunk import ChunkParserI\n",
    "\n",
    "\n",
    "#eigenen Chunk-Algorithmus umsetzen, basierend auf bi- und uni-gram taggern\n",
    "class NGramTagChunker(ChunkParserI):\n",
    "\n",
    "    def __init__(self, train_sentences,\n",
    "                 tagger_classes=[UnigramTagger, BigramTagger]):#beim Initialisieren wird der Chunker trainiert\n",
    "        train_sent_tags = conll_tag_chunks(train_sentences)#pos-tags und chunk-tags extrahieren\n",
    "        self.chunk_tagger = combined_tagger(train_sent_tags, tagger_classes)#chunk-tagger erstellen/trainieren\n",
    "\n",
    "    def parse(self, tagged_sentence):#wendet den trainierten Chunk-Tagger auf neuen Satz an - Vorraussetzung: POS-Tags\n",
    "        if not tagged_sentence:\n",
    "            return None\n",
    "        pos_tags = [tag for word, tag in tagged_sentence]#POS-Tags extrahieren/isolieren\n",
    "        chunk_pos_tags = self.chunk_tagger.tag(pos_tags)#chunk-tags ermitteln, zusätzlich zu pos-tags\n",
    "        chunk_tags = [chunk_tag for (pos_tag, chunk_tag) in chunk_pos_tags]#chunk-tags isolieren (eigentlich nicht notwendig)\n",
    "        wpc_tags = [(word, pos_tag, chunk_tag) for ((word, pos_tag), chunk_tag)\n",
    "                    in zip(tagged_sentence, chunk_tags)]#tag-tripel herstellen als Grundlage für die Parsebaum-Erstellung\n",
    "        return conlltags2tree(wpc_tags)#Parsebaum zurückgeben\n",
    "\n",
    "\n",
    "# train the shallow parser\n",
    "ntc = NGramTagChunker(train_data)\n",
    "\n",
    "# test parser performance on test data\n",
    "print(f\"Evaluation auf Testdaten: \\n{ntc.evaluate(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6da82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse our sample sentence\n",
    "import spacy#wird nur für das pos-tagging gebraucht\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "sentence_nlp = nlp(sentence)#pos-tags ermitteln\n",
    "tagged_sentence = [(word.text, word.tag_) for word in sentence_nlp]#eingabeformat für chunker erstellen\n",
    "tree = ntc.parse(tagged_sentence)#beispielsatz parsen\n",
    "print(f\"Unser Beispielsatz: \\n{tree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edb97fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#umfangreicherer Korpus, enthält Artikel des Wall Street Journals (wsj)\n",
    "from nltk.corpus import conll2000\n",
    "\n",
    "wsj_data = conll2000.chunked_sents()\n",
    "train_wsj_data = wsj_data[:10000]\n",
    "test_wsj_data = wsj_data[10000:]\n",
    "# look at a sample sentence in the corpus\n",
    "print(f\"Beispielsatz aus dem WSJ-Korpus:\\n{train_wsj_data[10]}\")\n",
    "\n",
    "\n",
    "# train the shallow parser\n",
    "tc = NGramTagChunker(train_wsj_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9097f172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test performance on the test data\n",
    "print(f\"Evaluation des neu trainierten Chunkers:\\n{tc.evaluate(test_wsj_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592d1d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse our sample sentence\n",
    "tree = tc.parse(tagged_sentence)\n",
    "print(f\"Unser Beispielsatz mit neuem Chunker:\\n{tree}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de724a7d",
   "metadata": {},
   "source": [
    "### Feature Engineering (NLP5.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edba807",
   "metadata": {},
   "source": [
    "Da Machine Learning Methoden auf Nummerik basieren,müssen textuelle Daten angepasst werden, um die Verfahren zu nutzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca791728",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ausschnitte aus dem Code von Sarkar\n",
    "# teils korrigiert und ergänzt - siehe Anmerkungen und Kommentare\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d05a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.options.display.max_colwidth = 200\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851e8203",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['The sky is blue and beautiful.',\n",
    "          'Love this blue and beautiful sky!',\n",
    "          'The quick brown fox jumps over the lazy dog.',\n",
    "          \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\",\n",
    "          'I love green eggs, ham, sausages and bacon!',\n",
    "          'The brown fox is quick and the blue dog is lazy!',\n",
    "          'The sky is very blue and the sky is very beautiful today',\n",
    "          'The dog is lazy but the brown fox is quick!'\n",
    "]\n",
    "labels = ['weather', 'weather', 'animals', 'food', 'food', 'animals', 'weather', 'animals']\n",
    "\n",
    "corpus = np.array(corpus)\n",
    "corpus_df = pd.DataFrame({'Document': corpus,\n",
    "                          'Category': labels})\n",
    "corpus_df = corpus_df[['Document', 'Category']]\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaf05f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6e0c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_corpus = normalize_corpus(corpus)\n",
    "norm_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac9ac95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ohne vorher die Wörter zu zählen...\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tv = TfidfVectorizer(min_df=0., max_df=1., norm='l2',\n",
    "                     use_idf=True, smooth_idf=True)\n",
    "tv_matrix = tv.fit_transform(norm_corpus)\n",
    "tv_matrix = tv_matrix.toarray()\n",
    "\n",
    "vocab = tv.get_feature_names()\n",
    "pd.DataFrame(np.round(tv_matrix, 2), columns=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d8932e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nur zum Vergleich: so wurden vorher die Werte berechnet (Sarkar)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity(tv_matrix)\n",
    "similarity_df = pd.DataFrame(similarity_matrix)\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764a627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Similarity - anderer Algorithmus pdist\n",
    "#  dient als Eingabe für das linkage später\n",
    "#  Hintergrund: linkage\n",
    "\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "similarity_matrix_pdist = pdist(tv_matrix, \"cosine\")\n",
    "similarity_df_pdist = pd.DataFrame(similarity_matrix_pdist)\n",
    "similarity_df_pdist #Darstellung flat array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94200c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bessere Darstellung erwünscht\n",
    "from scipy.spatial.distance import squareform\n",
    "pd.DataFrame(squareform(similarity_matrix_pdist))\n",
    "\n",
    "#Wie man sieht, sind die Werte \"invertiert\" (1-x)\n",
    "# das sind die Abstände"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01281b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "#wir können wie besprochen \"ward\" nicht verwenden\n",
    "# also probieren wir eine andere Methode aus: single link\n",
    "\n",
    "Z = linkage(similarity_matrix_pdist, 'single')\n",
    "pd.DataFrame(Z, columns=['Document\\Cluster 1', 'Document\\Cluster 2',\n",
    "                         'Distance', 'Cluster Size'], dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca949845",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Data point')\n",
    "plt.ylabel('Distance')\n",
    "dendrogram(Z)\n",
    "plt.axhline(y=0.6, c='k', ls='--', lw=0.5)\n",
    "#plt.savefig(\"K:/Documents/Lehre/KE/cluster-pdist-single\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba8c733",
   "metadata": {},
   "source": [
    "### Spam Klassifikation mit Naive Bayes (NLP6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e65903",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listen mit den Dokumentnamen\n",
    "\n",
    "dnames = os.listdir(\"C:\\\\Users\\micou\\OneDrive\\Studium\\2021 FHGR\\2. Semester\\NLP\\Notebooks\\enron1\\enron1\\ham\")\n",
    "dnamesSP = os.listdir(\"C:\\\\Users\\micou\\OneDrive\\Studium\\2021 FHGR\\2. Semester\\NLP\\Notebooks\\enron1\\enron1\\ham\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05fac00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dokumente einlesen\n",
    "\n",
    "maillist = []\n",
    "\n",
    "for dat in dnames: \n",
    "    with open(\"C:\\\\Users\\micou\\OneDrive\\Studium\\2021 FHGR\\2. Semester\\NLP\\Notebooks\\enron1\\enron1\\ham\\\" + dat, encoding = 'utf-8') as f:\n",
    "        mail = f.read()\n",
    "        maillist.append(mail)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79452378",
   "metadata": {},
   "outputs": [],
   "source": [
    "maillistSPAM = []        \n",
    "\n",
    "for dat in dnamesSP: \n",
    "\n",
    "    with open(\"C:\\\\Users\\micou\\OneDrive\\Studium\\2021 FHGR\\2. Semester\\NLP\\Notebooks\\enron1\\enron1\\ham\\\" + dat, encoding = 'utf-8') as f:\n",
    "        mail = f.read()\n",
    "        maillistSPAM.append(mail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4124769",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalisieren der Mails\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d7f27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_document(doc):\n",
    "\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_mails = np.vectorize(normalize_document)\n",
    "norm_mails = normalize_mails(maillist)\n",
    "norm_SPAM = normalize_mails(maillistSPAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89667ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing\n",
    "mail_words = []\n",
    "GAP_PATTERN = r'\\s+'\n",
    "regex_wt = nltk.RegexpTokenizer(pattern=GAP_PATTERN, gaps=True)\n",
    "  \n",
    "for mail in norm_mails:\n",
    "    words = regex_wt.tokenize(mail)\n",
    "    mail_words.append(words)\n",
    "\n",
    "SPAM_words = []\n",
    "\n",
    "for mail in norm_SPAM:\n",
    "    words = regex_wt.tokenize(mail)\n",
    "    SPAM_words.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9de2f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste für den Classifier erstellen\n",
    "list_dic = []\n",
    "\n",
    "for mail in mail_words:\n",
    "    dic = {}\n",
    "    for word in mail:\n",
    "        dic[word] = True\n",
    "    list_dic.append((dic, \"ham\"))      \n",
    "list_dic\n",
    "\n",
    "for mail in norm_SPAM:\n",
    "    dic = {}\n",
    "    for word in mail:\n",
    "        dic[word] = True\n",
    "    list_dic.append((dic, \"spam\"))  \n",
    "\n",
    "list_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b31bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ham und spam mischen \n",
    "\n",
    "import random\n",
    "random.shuffle(list_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4d1983",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trainings- und Testdaten teilen\n",
    "# print(len(list_dic)) = 5172\n",
    "\n",
    "training = list_dic[0:4000]\n",
    "test = list_dic[4000:]\n",
    "\n",
    "from nltk import NaiveBayesClassifier, classify\n",
    "classifier = NaiveBayesClassifier.train(training)\n",
    "classify.accuracy(classifier, test)\n",
    "#Resultat = 1.0 --> zu hoch\n",
    "\n",
    "classifier.show_most_informative_features()\n",
    "\n",
    "#Ausgabe der features ergibt, dass der Classifier vor allem einzelne Buchstaben als Indiz für Spam genommen hat. \n",
    "#Eine kurze Recherche ergab, dass es beim Tokenizing wohl noch ein paar Fehler gibt, da dort einzelne Buchstaben als Wörter extrahiert wurden. \n",
    "#Tokenizer müsste also noch optimiert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cfa7d9",
   "metadata": {},
   "source": [
    "### Übungsaufgaben"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf27ac35",
   "metadata": {},
   "source": [
    "Text normalisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdea7ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\"University of Applied Sciences of the Grisons is an innovative and entrepreneurial university of applied sciences with over 2,000 students. It trains people to become responsible and skilled professionals and managers. As a university of applied sciences with strong regional roots, University of Applied Sciences of the Grisons attracts students from beyond the canton and even from outside Switzerland with its welcoming atmosphere. University of Applied Sciences of the Grisons offers a range of bachelor’s, master’s and further education programmes in Architecture, Civil Engineering, Computational and Data Science, Digital Science, Digital Supply Chain, Management, Mobile Robotics, Multimedia Production, Photonics, Service Design and Tourism. It also performs applied research in these disciplines and in doing so contributes to the development of innovations, knowledge and solutions for society. University of Applied Sciences of the Grisons has been part of the University of Applied Sciences of Eastern Switzerland (FHO) since 2000. Following the Federal Council’s recognition of University of Applied Sciences of the Grisons’s qualification for financial support, it will become Switzerland’s eighth public university of applied sciences from 1 January 2020. University of Applied Sciences of the Grisons’s history dates back to 1963 with the foundation of the 'Abendtechnikum Chur', a technical college of evening courses.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dac5fd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Lower case ok\n",
    "    #Special char removal ok\n",
    "    #Stopword removal ok\n",
    "    #Digit removal ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d742834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "33673a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'university applied sciences grisons innovative entrepreneurial university applied sciences students trains people become responsible skilled professionals managers university applied sciences strong regional roots university applied sciences grisons attracts students beyond canton even outside switzerland welcoming atmosphere university applied sciences grisons offers range bachelors masters education programmes architecture civil engineering computational data science digital science digital supply chain management mobile robotics multimedia production photonics service design tourism also performs applied research disciplines contributes development innovations knowledge solutions society university applied sciences grisons part university applied sciences eastern switzerland fho since following federal councils recognition university applied sciences grisonss qualification financial support become switzerlands eighth public university applied sciences january university applied sciences grisonss history dates back foundation abendtechnikum chur technical college evening courses'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalisierter_text = normalize_document(text)\n",
    "normalisierter_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391be290",
   "metadata": {},
   "source": [
    "häufigste Wörter zählen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "75ec207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python code to find frequency of each word\n",
    "def freq(str):\n",
    "    # break the string into list of words \n",
    "    str = str.split()         \n",
    "    str2 = []\n",
    "    # loop till string values present in list str\n",
    "    for i in str:             \n",
    "        # checking for the duplicacy\n",
    "        if i not in str2:\n",
    "            # insert value in str2\n",
    "            str2.append(i) \n",
    "       \n",
    "    for i in range(0, len(str2)):\n",
    "  \n",
    "        # count the frequency of each word(present \n",
    "        # in str2) in str and print\n",
    "        print(str.count(str2[i]), str2[i])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c4e1fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 university\n",
      "11 applied\n",
      "10 sciences\n",
      "4 grisons\n",
      "1 innovative\n",
      "1 entrepreneurial\n",
      "2 students\n",
      "1 trains\n",
      "1 people\n",
      "2 become\n",
      "1 responsible\n",
      "1 skilled\n",
      "1 professionals\n",
      "1 managers\n",
      "1 strong\n",
      "1 regional\n",
      "1 roots\n",
      "1 attracts\n",
      "1 beyond\n",
      "1 canton\n",
      "1 even\n",
      "1 outside\n",
      "2 switzerland\n",
      "1 welcoming\n",
      "1 atmosphere\n",
      "1 offers\n",
      "1 range\n",
      "1 bachelors\n",
      "1 masters\n",
      "1 education\n",
      "1 programmes\n",
      "1 architecture\n",
      "1 civil\n",
      "1 engineering\n",
      "1 computational\n",
      "1 data\n",
      "2 science\n",
      "2 digital\n",
      "1 supply\n",
      "1 chain\n",
      "1 management\n",
      "1 mobile\n",
      "1 robotics\n",
      "1 multimedia\n",
      "1 production\n",
      "1 photonics\n",
      "1 service\n",
      "1 design\n",
      "1 tourism\n",
      "1 also\n",
      "1 performs\n",
      "1 research\n",
      "1 disciplines\n",
      "1 contributes\n",
      "1 development\n",
      "1 innovations\n",
      "1 knowledge\n",
      "1 solutions\n",
      "1 society\n",
      "1 part\n",
      "1 eastern\n",
      "1 fho\n",
      "1 since\n",
      "1 following\n",
      "1 federal\n",
      "1 councils\n",
      "1 recognition\n",
      "2 grisonss\n",
      "1 qualification\n",
      "1 financial\n",
      "1 support\n",
      "1 switzerlands\n",
      "1 eighth\n",
      "1 public\n",
      "1 january\n",
      "1 history\n",
      "1 dates\n",
      "1 back\n",
      "1 foundation\n",
      "1 abendtechnikum\n",
      "1 chur\n",
      "1 technical\n",
      "1 college\n",
      "1 evening\n",
      "1 courses\n"
     ]
    }
   ],
   "source": [
    "freq(normalisierter_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672c4ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
