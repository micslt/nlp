{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec6eb192",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f524d0bd",
   "metadata": {},
   "source": [
    "### Syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ead40c",
   "metadata": {},
   "source": [
    "- Entstehung von Struktur zwischen Wörtern (=Syntax) mit eigener Bedeutung\n",
    "(über die Summe der Teile hinaus)\n",
    "- Einordnung und Struktur\n",
    "- Einordnung oder Kategorisierung von Sätzen, Phrasen, Wörtern usw.\n",
    "- Strukturierung/Ordnung/Reihenfolge: Kombination zu jeweils längeren Blöcken\n",
    "- Gesucht wird Strukturierung der Wörter.\n",
    "- Oft: Hierarchische Strukturierung\n",
    "• Satz\n",
    "• Satzteil (Clause)\n",
    "• Phrase\n",
    "• Wort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e4c539",
   "metadata": {},
   "source": [
    "### Semantik"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b97d93",
   "metadata": {},
   "source": [
    "- Bedeutung von Sprache\n",
    "- Lexikalische Semantik:               Bedeutung von Wörtern oder Symbolen\n",
    "- Kompositionelle Semantik:            Beziehungen zwischen Wörtern\n",
    "- Bedeutung von Wortfolgen und Sätzen\n",
    "- Beziehungen zwischen Sätzen\n",
    "\n",
    "#### Morphem:\n",
    "\n",
    "- Baut oft statt auf Wörtern auf Morphemen auf\n",
    "- Kleinste bedeutungsvolle Einheit\n",
    "- Wort kann aus mehreren Morphemen bestehen\n",
    "- Zusammengesetzte Wörter\n",
    "- Vor- und Nachsilben\n",
    "- Ergänzungen für Plural usw.\n",
    "- Bedeutung ergibt sich aus der Kombination der Einzelbedeutungen:\n",
    "- Beispiel: Tische: {Tisch}{e} -> bestimmtes Möbelstück in der Mehrzahl\n",
    "\n",
    "#### Lexem:\n",
    "- Sprachliche Bedeutungseinheit\n",
    "- Abstrahiert von Aussprache, Syntax usw.\n",
    "- Beispiel: Flexionsformen eines Verbs gehören zum selben Lexem singen, singt, singst\n",
    "- (uneinheitliche Definitionen)\n",
    "\n",
    "#### Lemma:\n",
    "- Grundform, Lexikoneintrag\n",
    "- Für deutsche Nomen bspw. Nominativ Singular: Lied, Baum, Vorlesung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e292ab9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad064c30",
   "metadata": {},
   "source": [
    "### Grammatik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96ed8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Zusatzuebung aus Termin 1 NLP aus Vorlesung\n",
    "Erweitern Sie die Grammatik um Haupt - und Nebensatz.\n",
    "Beispielsatz: the brown fox is quick and he is jumping over the lazy dog\n",
    "\n",
    "https://www.nltk.org/howto/grammar.html\n",
    "\"\"\"\n",
    "\n",
    "import nltk\n",
    "\n",
    "#Start\n",
    "grammar2 = nltk.CFG.fromstring(\"\"\"\n",
    "  S  -> NP | NP VP\n",
    "  NP -> DET ADJ N PP | DET N | DET ADJ N | DET ADJ N PP \n",
    "  VP -> V\n",
    "  PP -> PREP NP\n",
    "  N -> 'fox' | 'dog' | 'cat' | 'box'\n",
    "  DET -> 'the' | 'a'\n",
    "  ADJ  -> 'quick' | 'brown' |  'lazy' | 'tall'\n",
    "  V ->  'is'  | 'jumping' | 'running'\n",
    "  PREP -> 'in'\n",
    "  \"\"\")\n",
    "\n",
    "# words = \"the brown fox\".split()\n",
    "words = \"the lazy cat in a box\".split()\n",
    "my_parser = nltk.RecursiveDescentParser(grammar2)  # (grammar2, trace=2) falls keine Ausgabe kommt...\n",
    "for tree in my_parser.parse(words):\n",
    "    print(tree)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820bbe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Erweitern sie das Programmierbeispiel von eben, indem sie weitere Sätze finden, \n",
    "die von der unveränderten Grammatik geparst werden können. Finden sie mindestens \n",
    "drei weitere Sätze und stellen sie diese mit dem Parse-Baum hier ein.\n",
    "\n",
    "Tipp: Wenn keine Ausgabe kommt, ist ein Parse-Fehler aufgetreten. \n",
    "Sie können wie im Python-Beispiel auskommentiert das tracing einschalten, um mehr Hinweise zu bekommen.\n",
    "\"\"\"\n",
    "\n",
    "grammar2 = nltk.CFG.fromstring(\"\"\"\n",
    "  S  -> NP | NP VP PP | NP VP\n",
    "  NP -> DET ADJ N PP | DET N | DET ADJ N \n",
    "  VP -> V | V V\n",
    "  PP -> PREP NP\n",
    "  N -> 'fox' | 'dog' | 'cat' | 'box'\n",
    "  DET -> 'the' | 'a'\n",
    "  ADJ  -> 'quick' | 'brown' |  'lazy' | 'tall'\n",
    "  V ->  'is'  | 'jumping' | 'running'\n",
    "  PREP -> 'in'\n",
    "  \"\"\")\n",
    "# words = \"the brown fox\".split()\n",
    "#words = \"the brown fox in a box\".split()\n",
    "#words = \"a quick cat running\".split()\n",
    "#words = \"the quick fox is in the brown box\".split()\n",
    "words = \"the quick fox is jumping\".split()\n",
    "my_parser = nltk.RecursiveDescentParser(grammar2) # (grammar2, trace=2) falls keine Ausgabe kommt...\n",
    "for tree in my_parser.parse(words):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b27a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Zusatzuebung aus Termin 1 NLP aus Vorlesung\n",
    "Erweitern Sie die Grammatik um Haupt - und Nebensatz.\n",
    "Beispielsatz: the brown fox is quick and he is jumping over the lazy dog\n",
    "\n",
    "https://www.nltk.org/howto/grammar.html\n",
    "\"\"\"\n",
    "grammar2 = nltk.CFG.fromstring(\"\"\" \n",
    "  S   -> HS CCNJ NS | HS\n",
    "  HS  -> NP | NP VP | NP VP PP | NP VP ADJ\n",
    "  NS  -> PRP VP PREP NP\n",
    "  NP -> DET ADJ N PP | DET N | DET ADJ N\n",
    "  PRP -> PR\n",
    "  VP -> V | V V\n",
    "  PP -> PREP NP\n",
    "  N -> 'fox' | 'dog' | 'cat' | 'box'\n",
    "  DET -> 'the' | 'a'\n",
    "  ADJ  -> 'quick' | 'brown' |  'lazy' | 'tall' | \"blue\"\n",
    "  V ->  'is'  | 'jumping' | 'running' | \"jumps\"\n",
    "  PREP -> 'in' | \"over\"\n",
    "  PR  -> \"he\" | \"she\" | \"it\"\n",
    "  CCNJ -> \"and\" | \"but\" | \"that\" | \"if\"\n",
    "  \"\"\")\n",
    "words = \"the brown fox is quick and he is jumping over the lazy dog\".split()\n",
    "my_parser = nltk.RecursiveDescentParser(grammar2)  # (grammar2, trace=2) falls keine Ausgabe kommt...\n",
    "for tree in my_parser.parse(words):  # man muss nicht unbedingt alle trees finden, manchmal genügt auch einer!!\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eb45b9",
   "metadata": {},
   "source": [
    "### Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f29ac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67182c93",
   "metadata": {},
   "source": [
    "Syntax\n",
    "https://docs.python.org/3/library/re.html#regular-expression-syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f3c83e",
   "metadata": {},
   "source": [
    "- \\d Matches any decimal digits (0,1,2,3,4,5,6,7,8,9).\n",
    "- \\D Matches any character that is not a decimal digit.\n",
    "- \\s Matches any whitespace character such as space, tab and newline.\n",
    "- \\S Matches any character that is not a whitespace.\n",
    "- \\w Matches any \"word\" character (letter, digit or the underscore).\n",
    "- \\W Matches any character that is not a \"word\" character.\n",
    "- [...] Matches any character that listed inside square brackets. For example [abc] matches one character that is either a, b or c.\n",
    "- [^...] Matches any character that is not listed inside square brackets. For example [^abc] matches one character that is NOT a, b or c.\n",
    "- [0-9] Matches any character between 0 and 9. It is equivalent to using \\d.\n",
    "- [A-D] Matches any character between A and F (A, B, C, D).\n",
    "\n",
    "\n",
    "- \" + \" Matches the previous item one or more times. For example, A+ will match strings such as A, AA, AAA and etc.\n",
    "- \" ? \" Matches the previous item zero or one time. It is used to match an optional part of the pattern.\n",
    "- \" * \" Matches the previous item zero or more time. It is used to match optional parts of the pattern.\n",
    "- {n} Matches the previous item exactly n times. For example, A{2} will match strings such as AA.\n",
    "- {n,m} Matches the previous item at least n times, but no more than m times. For example, A{2,5} will match strings such as AA, AAA, AAAA or AAAAA.\n",
    "- {n,} Matches the previous item at least n or more times. For example, A{2,} will match strings such as AA, AAA, AAAA, AAAAA and so on.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78fc404",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Probieren sie die Beispiele zu regulären Ausdrücken aus (Code von Sarkar, Ch2b).\n",
    "Bilden sie einen regulären Ausdruck für Telefonnummern und testen sie diesen mittels selbstgewählter Beispiele. \n",
    "Sie sollen als Ziel (potentielle) Telefonnummern in beliebigem anderem Text erkennen und ausgeben können.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "string = \"Hallo, meine Telefonnummer ist 0765227843  [Titel anhand dieser ISBN in Citavi-Projekt übernehmen]  . International ist es +41765227843 blah +41 76 522 78 43 Ich wohne in 0800 Zürich. Foo bar \" \\\n",
    "         \"076 522 78 43, blah 435345300555 +41(0)765227843 und 0041765527843\"\n",
    "\n",
    "pattern = '0+\\d{2}\\s?\\d{3}\\s?\\d{2}\\s?\\d{2}|\\+\\d{2}\\s?\\d{2}\\s?\\d{3}\\s?\\d{2}\\s?\\d{2}'\n",
    "pattern2 = '(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})'\n",
    "pattern3 = '((?:\\+\\d{2}[-\\.\\s]??|\\d{4}[-\\.\\s]??)?(?:\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4}))'\n",
    "\n",
    "\n",
    "print(re.findall(pattern, string))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c533a4b",
   "metadata": {},
   "source": [
    "### Übungsaufgabe Web-Scraping\n",
    "\n",
    "Wir wollen das Web-Scraping auf einen eigenen Text anwenden und diesen entsprechend dem Vorbild aufbereiten. Dazu wählen wir die Bündner Verfassung. Diese ist beispielsweise hier erreichbar: http://www.verfassungen.ch/graubuenden/verf2003.htm (Bitte verwenden sie diese Quelle, auch wenn sie eine andere kennen sollten, damit wir alle auf der gleichen Grundlage arbeiten).\n",
    "\n",
    "    Quelltext/HTML herunterladen\n",
    "    Reinen Text extrahieren\n",
    "    (gut aufbewahren, brauchen wir noch...)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d71ae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text) #löscht Umbrüche und so Sachen\n",
    "    return stripped_text\n",
    "\n",
    "data = requests.get('https://gastrosocial.ch/de/arbeitgeber/versicherungsangebot/uebersicht-versicherungsangebot')\n",
    "content = data.content\n",
    "\n",
    "\n",
    "\n",
    "clean_content = strip_html_tags(content)\n",
    "print(clean_content[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31420de",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3980162",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9e7cb7",
   "metadata": {},
   "source": [
    "#### Verfahren:\n",
    "- Porter Stemmer\n",
    "- Lancaster Stemmer\n",
    "- Regex based Stemmer\n",
    "- Snowball Stemmer\n",
    "##### Vor/Nachteile\n",
    "- Vorteil: regelbasiert, einfach, muss kein Wörterbuch aufgebaut werden\n",
    "- Nachteil: ungenauer als Lemmatisierung\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a5f30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porter Stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "print(ps.stem('jumping'), ps.stem('jumps'), ps.stem('jumped'))\n",
    "print(ps.stem('lying'))\n",
    "print(ps.stem('strange'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96991a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lancaster Stemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "ls = LancasterStemmer()\n",
    "\n",
    "print(ls.stem('jumping'), ls.stem('jumps'), ls.stem('jumped'))\n",
    "print(ls.stem('lying'))\n",
    "print(ls.stem('strange'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c03a595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex based stemmer\n",
    "from nltk.stem import RegexpStemmer\n",
    "rs = RegexpStemmer('ing$|s$|ed$', min=4)\n",
    "print(rs.stem('jumping'), rs.stem('jumps'), rs.stem('jumped'))\n",
    "print(rs.stem('lying'))\n",
    "print(rs.stem('strange'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef73d959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowball Stemmer (auch für Deutsch, aber schneided nur Endung ab, \n",
    "# Stemmming geht nicht gut in Deutsch, besser lemmatisierung)\n",
    "from nltk.stem import SnowballStemmer\n",
    "ss = SnowballStemmer(\"german\")\n",
    "print('Supported Languages:', SnowballStemmer.languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c2b0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming on German words\n",
    "# autobahnen -> cars\n",
    "# autobahn -> car\n",
    "print(ss.stem('autobahnen'))\n",
    "\n",
    "# springen -> jumping\n",
    "# spring -> jump\n",
    "print(ss.stem('springen'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5281f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_stemmer(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "simple_stemmer(\"My system keeps crashing his crashed yesterday, ours crashes daily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e225b35b",
   "metadata": {},
   "source": [
    "### Lemmatisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef83751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf66de5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize nouns\n",
    "print(wnl.lemmatize('cars', 'n'))\n",
    "print(wnl.lemmatize('men', 'n'))\n",
    "\n",
    "# lemmatize verbs\n",
    "print(wnl.lemmatize('running', 'v'))\n",
    "print(wnl.lemmatize('ate', 'v'))\n",
    "\n",
    "# lemmatize adjectives\n",
    "print(wnl.lemmatize('saddest', 'a'))\n",
    "print(wnl.lemmatize('fancier', 'a'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e3bd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# use spacy.load('en') if you have downloaded the language model en directly after install spacy\n",
    "nlp = spacy.load('en_core', parse=True, tag=True, entity=True)\n",
    "text = 'My system keeps crashing his crashed yesterday, ours crashes daily'\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "lemmatize_text(\"My system keeps crashing! his crashed yesterday, ours crashes daily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e391491",
   "metadata": {},
   "source": [
    "### Stopwörter entfernen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbb7cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text, is_lower_case=False, stopwords=stopword_list):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "remove_stopwords(\"The, and, if are stopwords, computer is not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5b5ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = (\"US unveils world's most powerful supercomputer, beats China. \" \n",
    "               \"The US has unveiled the world's most powerful supercomputer called 'Summit', \" \n",
    "               \"beating the previous record-holder China's Sunway TaihuLight. With a peak performance \"\n",
    "               \"of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, \"\n",
    "               \"which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, \"\n",
    "               \"which reportedly take up the size of two tennis courts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eff8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "remove_accented_chars('Sómě Áccěntěd těxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3d5a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contractions import CONTRACTION_MAP\n",
    "import re\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48abe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=True, \n",
    "                     text_lemmatization=True, special_char_removal=True, \n",
    "                     stopword_removal=True, remove_digits=True):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "        # strip HTML\n",
    "        if html_stripping:\n",
    "            doc = strip_html_tags(doc)\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "        # expand contractions    \n",
    "        if contraction_expansion:\n",
    "            doc = expand_contractions(doc)\n",
    "        # lowercase the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        # remove extra newlines\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "        # remove special characters and\\or digits    \n",
    "        if special_char_removal:\n",
    "            # insert spaces between special characters to isolate them    \n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "            \n",
    "        normalized_corpus.append(doc)\n",
    "        \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5391ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'Original': sample_text,\n",
    " 'Processed': normalize_corpus([sample_text])[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1965a208",
   "metadata": {},
   "source": [
    "### Übung Normalisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6ec37a",
   "metadata": {},
   "source": [
    "Wenden sie den kompletten Normalisierungsablauf auf unseren Beispieltext an (Bündner Verfassung).\n",
    "\n",
    "    Gehen sie schrittweise vor und überprüfen sie, ob es Änderungen gibt und falls ja, welche.\n",
    "    Ändern sie die Einstellungen, wo notwendig oder sinnvoll\n",
    "    Dokumentieren sie ihre Schritte \n",
    "    Wo ist noch Verbesserungsbedarf?\n",
    "    Insbesondere natürlich für die Umstellung von Englisch auf Deutsch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8a884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"de_core_news_sm\")  # für Deutsch statt Englisch (en_core_web_sm) inkl. Installation\n",
    "verfassung = content  # Eindeutige Bezeichnung für unsere Textdaten\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "print(lemmatize_text(\"My system keeps crashing! his crashed yesterday, ours crashes daily\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363e8009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False, stopwords=stopword_list):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "print(remove_stopwords(\"The, and, if are stopwords, computer is not\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f4e823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=True,\n",
    "                     text_lemmatization=True, special_char_removal=True,\n",
    "                     stopword_removal=True, remove_digits=True):\n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "        # strip HTML\n",
    "        if html_stripping:\n",
    "            doc = strip_html_tags(doc)\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "        # expand contractions\n",
    "        if contraction_expansion:\n",
    "            doc = expand_contractions(doc)\n",
    "        # lowercase the text\n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        # remove extra newlines\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ', doc)\n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "        # remove special characters and\\or digits\n",
    "        if special_char_removal:\n",
    "            # insert spaces between special characters to isolate them\n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits)\n",
    "            # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "        normalized_corpus.append(doc)\n",
    "    return normalized_corpus\n",
    "\n",
    "sample_text = verfassung  # Abruf des deutschen Texts\n",
    "print('Original:', sample_text)\n",
    "print('Normalisiert:', normalize_corpus([sample_text])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0306fc0",
   "metadata": {},
   "source": [
    "### Pos Tagger (Wortarterkennung)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa07873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "sentence = \"US unveils world's most powerful supercomputer, beats China.\"\n",
    "nltk_pos_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "print(f\"Beispielsatz getaggt: {nltk_pos_tagged}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4411ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank\n",
    "data = treebank.tagged_sents()\n",
    "train_data = data[:3500]\n",
    "test_data = data[3500:]\n",
    "print(f\"Satz aus den Trainingsdaten: {train_data[0]}\")\n",
    "print(f\"Anzahl Sätze insgesamt: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3616a769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default tagger\n",
    "from nltk.tag import DefaultTagger\n",
    "dt = DefaultTagger('NN')#nur Nomen...\n",
    "\n",
    "# accuracy on test data - wir nutzen die eingebaute Methode evaluate...\n",
    "print(f\"Genauigkeit nur Nomen: {dt.evaluate(test_data)}\")\n",
    "\n",
    "# tagging our sample headline\n",
    "print(f\"Angewendet auf Beispielsatz: {dt.tag(nltk.word_tokenize(sentence))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73f2969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import RegexpTagger\n",
    "# define regex tag patterns\n",
    "patterns = [\n",
    "        (r'.*ing$', 'VBG'),               # gerunds\n",
    "        (r'.*ed$', 'VBD'),                # simple past\n",
    "        (r'.*es$', 'VBZ'),                # 3rd singular present\n",
    "        (r'.*ould$', 'MD'),               # modals\n",
    "        (r'.*\\'s$', 'NN$'),               # possessive nouns\n",
    "        (r'.*s$', 'NNS'),                 # plural nouns\n",
    "        (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers\n",
    "        (r'.*', 'NN')                     # nouns (default) ...\n",
    "]\n",
    "rt = RegexpTagger(patterns)\n",
    "# accuracy on test data\n",
    "print(f\"Genauigkeit Regex: {rt.evaluate(test_data)}\")\n",
    "# tagging our sample headline\n",
    "print(f\"Regex auf Beispielsatz: {rt.tag(nltk.word_tokenize(sentence))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b4790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## N gram taggers\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger\n",
    "from nltk.tag import TrigramTagger\n",
    "\n",
    "ut = UnigramTagger(train_data)\n",
    "bt = BigramTagger(train_data)\n",
    "tt = TrigramTagger(train_data)\n",
    "# testing performance of unigram tagger\n",
    "print(f\"Genauigkeit Unigram: {ut.evaluate(test_data)}\")\n",
    "print(f\"Beispielsatz: {ut.tag(nltk.word_tokenize(sentence))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5fad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing performance of bigram tagger\n",
    "print(f\"Genauigkeit Bigram: {bt.evaluate(test_data)}\")\n",
    "print(f\"Beispielsatz: {bt.tag(nltk.word_tokenize(sentence))}\")\n",
    "input(\"Warte...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4969f8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing performance of trigram tagger\n",
    "print(f\"Genauigkeit Trigram: {tt.evaluate(test_data)}\")\n",
    "print(f\"Beispielsatz: {tt.tag(nltk.word_tokenize(sentence))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be2ebeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_tagger(train_data, taggers, backoff=None):\n",
    "    for tagger in taggers:\n",
    "        backoff = tagger(train_data, backoff=backoff)\n",
    "    return backoff\n",
    "\n",
    "ct = combined_tagger(train_data=train_data,\n",
    "                     taggers=[UnigramTagger, BigramTagger, TrigramTagger],\n",
    "                     backoff=rt)\n",
    "\n",
    "\n",
    "# evaluating the new combined tagger with backoff taggers\n",
    "print(f\"Genauigkeit Combined: {ct.evaluate(test_data)}\")\n",
    "print(f\"Beispielsatz: {ct.tag(nltk.word_tokenize(sentence))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ec2575",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.tag.sequential import ClassifierBasedPOSTagger\n",
    "\n",
    "nbt = ClassifierBasedPOSTagger(train=train_data,\n",
    "                               classifier_builder=NaiveBayesClassifier.train)\n",
    "\n",
    "# evaluate tagger on test data and sample sentence\n",
    "print(f\"Genauigkeit ML/Bayes: {nbt.evaluate(test_data)}\")\n",
    "print(f\"Beispielsatz: {nbt.tag(nltk.word_tokenize(sentence))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5d5e95",
   "metadata": {},
   "source": [
    "### Shallow Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caffe72d",
   "metadata": {},
   "source": [
    "#### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "412bd05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beispielsatz aus Trainingsdaten: (S\n",
      "  (NP A/DT Lorillard/NNP spokewoman/NN)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  ``/``\n",
      "  (NP This/DT)\n",
      "  is/VBZ\n",
      "  (NP an/DT old/JJ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# Shallow Parsing or Chunking\n",
    "import nltk\n",
    "from nltk.corpus import treebank_chunk\n",
    "\n",
    "sentence = \"US unveils world's most powerful supercomputer, beats China.\"\n",
    "\n",
    "data = treebank_chunk.chunked_sents()\n",
    "\n",
    "train_data = data[:3500]\n",
    "test_data = data[3500:]\n",
    "# view sample data\n",
    "print(f\"Beispielsatz aus Trainingsdaten: {train_data[7]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c3ffb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags: [('US', 'NNP'), ('unveils', 'JJ'), ('world', 'NN'), (\"'s\", 'POS'), ('most', 'RBS'), ('powerful', 'JJ'), ('supercomputer', 'NN'), (',', ','), ('beats', 'VBZ'), ('China', 'NNP'), ('.', '.')]\n",
      "Regex NP chunking example: \n",
      "(S\n",
      "  (NP US/NNP)\n",
      "  (NP unveils/JJ world/NN)\n",
      "  's/POS\n",
      "  most/RBS\n",
      "  (NP powerful/JJ supercomputer/NN)\n",
      "  ,/,\n",
      "  beats/VBZ\n",
      "  (NP China/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.chunk import RegexpParser\n",
    "\n",
    "# get POS tagged sentence\n",
    "tagged_simple_sent = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "print('POS Tags:', tagged_simple_sent)\n",
    "\n",
    "# illustrate NP chunking based on explicit chunk patterns\n",
    "chunk_grammar = \"\"\"\n",
    "NP: {<DT>?<JJ>*<NN.*>}\n",
    "\"\"\"\n",
    "rc = RegexpParser(chunk_grammar)\n",
    "c = rc.parse(tagged_simple_sent)\n",
    "\n",
    "# print and view chunked sentence using chunking\n",
    "print(f\"Regex NP chunking example: \\n{c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128b53a1",
   "metadata": {},
   "source": [
    "#### Chinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "722b610e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk/chink example:\n",
      "(S\n",
      "  (NP US/NNP)\n",
      "  unveils/JJ\n",
      "  (NP world/NN 's/POS most/RBS)\n",
      "  powerful/JJ\n",
      "  (NP supercomputer/NN ,/,)\n",
      "  beats/VBZ\n",
      "  (NP China/NNP ./.))\n"
     ]
    }
   ],
   "source": [
    "# illustrate NP chunking based on explicit chink patterns\n",
    "chink_grammar = \"\"\"\n",
    "NP:\n",
    "    {<.*>+}             # Chunk everything as NP\n",
    "    }<VBZ|VBD|JJ|IN>+{  # Chink sequences of VBD/VBZ/JJ/IN\n",
    "\"\"\"\n",
    "rc = RegexpParser(chink_grammar)\n",
    "c = rc.parse(tagged_simple_sent)\n",
    "\n",
    "# print and view chunked sentence using chinking\n",
    "print(f\"Chunk/chink example:\\n{c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f89fc5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mehr Phrasen:\n",
      "(S\n",
      "  (NP US/NNP)\n",
      "  (NP unveils/JJ world/NN)\n",
      "  's/POS\n",
      "  (ADVP most/RBS)\n",
      "  (NP powerful/JJ supercomputer/NN)\n",
      "  ,/,\n",
      "  (VP beats/VBZ)\n",
      "  (NP China/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# create a more generic shallow parser\n",
    "grammar = \"\"\"\n",
    "NP: {<DT>?<JJ>?<NN.*>}  \n",
    "ADJP: {<JJ>}\n",
    "ADVP: {<RB.*>}\n",
    "PP: {<IN>}      \n",
    "VP: {<MD>?<VB.*>+}\n",
    "\"\"\"\n",
    "rc = RegexpParser(grammar)\n",
    "c = rc.parse(tagged_simple_sent)\n",
    "\n",
    "# print and view shallow parsed sample sentence\n",
    "print(f\"Mehr Phrasen:\\n{c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3c6b0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation auf Testdaten: \n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  46.1%%\n",
      "    Precision:     19.9%%\n",
      "    Recall:        43.3%%\n",
      "    F-Measure:     27.3%%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate parser performance on test data\n",
    "print(f\"Evaluation auf Testdaten: \\n{rc.evaluate(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75e8bbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nochmal unser Beispielsatz aus den Trainingsdaten:\n",
      "(S\n",
      "  (NP A/DT Lorillard/NNP spokewoman/NN)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  ``/``\n",
      "  (NP This/DT)\n",
      "  is/VBZ\n",
      "  (NP an/DT old/JJ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "#conll - Dateiformat im NLP, geht auf Conference on Natural Language Learning zurück\n",
    "from nltk.chunk.util import tree2conlltags, conlltags2tree\n",
    "\n",
    "# look at a sample training tagged sentence\n",
    "train_sent = train_data[7]\n",
    "print(f\"Nochmal unser Beispielsatz aus den Trainingsdaten:\\n{train_sent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fd27eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tripel aus Wort, Tag, Chunk-Tag für den Beispielsatz:\n",
      "[('A', 'DT', 'B-NP'), ('Lorillard', 'NNP', 'I-NP'), ('spokewoman', 'NN', 'I-NP'), ('said', 'VBD', 'O'), (',', ',', 'O'), ('``', '``', 'O'), ('This', 'DT', 'B-NP'), ('is', 'VBZ', 'O'), ('an', 'DT', 'B-NP'), ('old', 'JJ', 'I-NP'), ('story', 'NN', 'I-NP'), ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# get the (word, POS tag, Chunk tag) triples for each token\n",
    "wtc = tree2conlltags(train_sent)\n",
    "print(f\"Tripel aus Wort, Tag, Chunk-Tag für den Beispielsatz:\\n{wtc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d60f465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zusammenführung der Tripel zu Parse-Baum:\n",
      "(S\n",
      "  (NP A/DT Lorillard/NNP spokewoman/NN)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  ``/``\n",
      "  (NP This/DT)\n",
      "  is/VBZ\n",
      "  (NP an/DT old/JJ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# get shallow parsed tree back from the WTC triples\n",
    "tree = conlltags2tree(wtc)\n",
    "print(f\"Zusammenführung der Tripel zu Parse-Baum:\\n{tree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c730009a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tags und chunk-tags extrahieren, wörter verwerfen\n",
    "def conll_tag_chunks(chunk_sents):\n",
    "    tagged_sents = [tree2conlltags(tree) for tree in chunk_sents]\n",
    "    return [[(t, c) for (w, t, c) in sent] for sent in tagged_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6732539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tagger sequentiell kombinieren (wie im POS-Beispiel)\n",
    "def combined_tagger(train_data, taggers, backoff=None):\n",
    "    for tagger in taggers:\n",
    "        backoff = tagger(train_data, backoff=backoff)\n",
    "    return backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "446f6b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation auf Testdaten: \n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  97.2%%\n",
      "    Precision:     91.4%%\n",
      "    Recall:        94.3%%\n",
      "    F-Measure:     92.8%%\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import UnigramTagger, BigramTagger\n",
    "from nltk.chunk import ChunkParserI\n",
    "\n",
    "\n",
    "#eigenen Chunk-Algorithmus umsetzen, basierend auf bi- und uni-gram taggern\n",
    "class NGramTagChunker(ChunkParserI):\n",
    "\n",
    "    def __init__(self, train_sentences,\n",
    "                 tagger_classes=[UnigramTagger, BigramTagger]):#beim Initialisieren wird der Chunker trainiert\n",
    "        train_sent_tags = conll_tag_chunks(train_sentences)#pos-tags und chunk-tags extrahieren\n",
    "        self.chunk_tagger = combined_tagger(train_sent_tags, tagger_classes)#chunk-tagger erstellen/trainieren\n",
    "\n",
    "    def parse(self, tagged_sentence):#wendet den trainierten Chunk-Tagger auf neuen Satz an - Vorraussetzung: POS-Tags\n",
    "        if not tagged_sentence:\n",
    "            return None\n",
    "        pos_tags = [tag for word, tag in tagged_sentence]#POS-Tags extrahieren/isolieren\n",
    "        chunk_pos_tags = self.chunk_tagger.tag(pos_tags)#chunk-tags ermitteln, zusätzlich zu pos-tags\n",
    "        chunk_tags = [chunk_tag for (pos_tag, chunk_tag) in chunk_pos_tags]#chunk-tags isolieren (eigentlich nicht notwendig)\n",
    "        wpc_tags = [(word, pos_tag, chunk_tag) for ((word, pos_tag), chunk_tag)\n",
    "                    in zip(tagged_sentence, chunk_tags)]#tag-tripel herstellen als Grundlage für die Parsebaum-Erstellung\n",
    "        return conlltags2tree(wpc_tags)#Parsebaum zurückgeben\n",
    "\n",
    "\n",
    "# train the shallow parser\n",
    "ntc = NGramTagChunker(train_data)\n",
    "\n",
    "# test parser performance on test data\n",
    "print(f\"Evaluation auf Testdaten: \\n{ntc.evaluate(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be6da82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unser Beispielsatz: \n",
      "(S\n",
      "  (NP US/NNP)\n",
      "  unveils/VBZ\n",
      "  (NP world/NN 's/POS most/RBS powerful/JJ supercomputer/NN)\n",
      "  ,/,\n",
      "  beats/VBZ\n",
      "  (NP China/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# parse our sample sentence\n",
    "import spacy#wird nur für das pos-tagging gebraucht\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "sentence_nlp = nlp(sentence)#pos-tags ermitteln\n",
    "tagged_sentence = [(word.text, word.tag_) for word in sentence_nlp]#eingabeformat für chunker erstellen\n",
    "tree = ntc.parse(tagged_sentence)#beispielsatz parsen\n",
    "print(f\"Unser Beispielsatz: \\n{tree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4edb97fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beispielsatz aus dem WSJ-Korpus:\n",
      "(S\n",
      "  (NP He/PRP)\n",
      "  (VP reckons/VBZ)\n",
      "  (NP the/DT current/JJ account/NN deficit/NN)\n",
      "  (VP will/MD narrow/VB)\n",
      "  (PP to/TO)\n",
      "  (NP only/RB #/# 1.8/CD billion/CD)\n",
      "  (PP in/IN)\n",
      "  (NP September/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "#umfangreicherer Korpus, enthält Artikel des Wall Street Journals (wsj)\n",
    "from nltk.corpus import conll2000\n",
    "\n",
    "wsj_data = conll2000.chunked_sents()\n",
    "train_wsj_data = wsj_data[:10000]\n",
    "test_wsj_data = wsj_data[10000:]\n",
    "# look at a sample sentence in the corpus\n",
    "print(f\"Beispielsatz aus dem WSJ-Korpus:\\n{train_wsj_data[10]}\")\n",
    "\n",
    "\n",
    "# train the shallow parser\n",
    "tc = NGramTagChunker(train_wsj_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9097f172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation des neu trainierten Chunkers:\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  89.1%%\n",
      "    Precision:     80.3%%\n",
      "    Recall:        86.1%%\n",
      "    F-Measure:     83.1%%\n"
     ]
    }
   ],
   "source": [
    "# test performance on the test data\n",
    "print(f\"Evaluation des neu trainierten Chunkers:\\n{tc.evaluate(test_wsj_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "592d1d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unser Beispielsatz mit neuem Chunker:\n",
      "(S\n",
      "  (NP US/NNP)\n",
      "  (VP unveils/VBZ)\n",
      "  (NP world/NN)\n",
      "  (NP 's/POS most/RBS powerful/JJ supercomputer/NN)\n",
      "  ,/,\n",
      "  (VP beats/VBZ)\n",
      "  (NP China/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# parse our sample sentence\n",
    "tree = tc.parse(tagged_sentence)\n",
    "print(f\"Unser Beispielsatz mit neuem Chunker:\\n{tree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a13bb64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
